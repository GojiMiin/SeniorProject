{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.layers import LayerNormalization,BatchNormalization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, GRU, Dropout, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.models import model_from_yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(filePath):\n",
    "    lemma_result = pd.read_csv(filePath)\n",
    "    print(lemma_result.shape)\n",
    "    return lemma_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkLength(max_feat, file):\n",
    "    leng = 0\n",
    "    sentence = []\n",
    "    allData = file['cleaned_review']\n",
    "\n",
    "    max_fatures = max_feat\n",
    "    tokenizer = Tokenizer(num_words=max_fatures, split=' ') \n",
    "    tokenizer.fit_on_texts(allData.values)\n",
    "    X1 = tokenizer.texts_to_sequences(allData.values)\n",
    "    \n",
    "    for i in X1:\n",
    "        if len(i)>leng:\n",
    "            leng = len(i)\n",
    "            sentence = i\n",
    "    print(leng)\n",
    "    print(sentence)\n",
    "    return leng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beforeCreateModel(max_feat,dataset,max_length):\n",
    "    max_fatures = max_feat #จำนวนคำที่ใช้ใน model\n",
    "    tokenizer = Tokenizer(num_words=max_fatures, split=' ') \n",
    "    tokenizer.fit_on_texts(dataset['cleaned_review'].values)\n",
    "    X1 = tokenizer.texts_to_sequences(dataset['cleaned_review'].values)\n",
    "    print(len(tokenizer.word_index))\n",
    "    feat = pad_sequences(X1, padding='pre',maxlen=max_length) # 505 = max_length in sentence\n",
    "    target = dataset['Label'].values\n",
    "\n",
    "    return feat,target,tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModelLSTM(embed_dim,lstm_out,max_feat,input_length):\n",
    "    embed_dim = embed_dim\n",
    "    lstm_out = lstm_out\n",
    "    model = Sequential() #Create Model\n",
    "    model.add(Embedding(input_dim = max_feat ,output_dim = embed_dim ,input_length = input_length)) #Input Layer\n",
    "    model.add(LSTM(lstm_out, dropout=0.2)) #1st hidden Layer\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1,activation='sigmoid')) # Output Layer\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModelGRU(embed_dim,gru_out,max_feat,input_length):\n",
    "    embed_dim = embed_dim\n",
    "    gru_out = gru_out\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = max_feat ,output_dim = embed_dim ,input_length = input_length)) #Input Layer\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(64, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=4))\n",
    "    model.add(GRU(100)) #1st hidden Layer\n",
    "    model.add(Dense(1,activation='sigmoid')) # Output Layer\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model,feat,target,validation_split,epochs,batch_size):\n",
    "    random.seed(10)\n",
    "    history = model.fit(feat, target,validation_split=validation_split, epochs = epochs, batch_size=batch_size)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(model,feat,target,batch_size):\n",
    "    score,acc = model.evaluate(feat, target, verbose = 2, batch_size = batch_size)\n",
    "    print(\"score: %.2f\" % (score))\n",
    "    print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLoss(history):\n",
    "    loss_values = history.history['loss']\n",
    "    val_loss_values = history.history['val_loss']\n",
    "    epochs = range(1, len(loss_values)+1)\n",
    "\n",
    "    plt.plot(epochs, loss_values, label='Training Loss')\n",
    "    plt.plot(epochs, val_loss_values, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(model):\n",
    "    # serialize model to YAML\n",
    "    model_yaml = model.to_yaml()\n",
    "    with open(\"Summary.yaml\", \"w\") as yaml_file:\n",
    "        yaml_file.write(model_yaml)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"Weights.h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadModel(yamlPathName, h5PathName):\n",
    "    with open(yamlPathName+'.yaml', 'r') as yaml_file:\n",
    "        print(yamlPathName)\n",
    "        loaded_model_yaml = yaml_file.read()\n",
    "        loaded_model = model_from_yaml(loaded_model_yaml)\n",
    "        loaded_model.load_weights(h5PathName+'.h5')\n",
    "    loaded_model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterAndShow(filter1, filter2, modelOutput, labelTest):\n",
    "    print(modelOutput)\n",
    "    test = []\n",
    "    test_y = []\n",
    "\n",
    "    for i in range(len(modelOutput)):\n",
    "        if(modelOutput[i] <filter1 or modelOutput[i]>filter2):\n",
    "            test.append(modelOutput[i])\n",
    "            test_y.append(labelTest[i])\n",
    "\n",
    "    test1 = np.array(test)\n",
    "    testy1 = np.array(test_y)\n",
    "    print(testy1)\n",
    "    print(test1.shape)\n",
    "    print(testy1.shape)\n",
    "    showConfusionMatrix(testy1, test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showConfusionMatrix(trueLabel,resultToShow):\n",
    "    labels = ['positive','negative']\n",
    "    cm = confusion_matrix(y_true=trueLabel , y_pred=resultToShow>0.5)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showWordWithCode(dataToMap, tokenizer): #dataToMap = list of sentiment\n",
    "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items())) # map id to all word in dic\n",
    "    print(reverse_word_map)\n",
    "    \n",
    "    def sequence_to_text(list_of_indices):\n",
    "        # Looking up words in dictionary\n",
    "        words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "        return(words)\n",
    "    \n",
    "    my_texts = list(map(sequence_to_text, dataToMap))\n",
    "    #my_texts\n",
    "    return reverse_word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveSentimentAndResult(sentenceToSave, resultToSave):\n",
    "    sen_temp = \"\"\n",
    "    SentimentSave = []\n",
    "    for one_sentence in sentenceToSave:\n",
    "        for word in one_sentence:\n",
    "            if isinstance(word, str):\n",
    "                sen_temp = sen_temp + \" \" + word\n",
    "            \n",
    "        SentimentSave.append(sen_temp)\n",
    "        sen_temp = \"\"\n",
    "            \n",
    "    #make 1 Dim predict result\n",
    "    resultSave = []\n",
    "    for arr_result in resultToSave:\n",
    "        for result in arr_result:\n",
    "            #print(result)\n",
    "            resultSave.append(result)\n",
    "            \n",
    "    data = {'lemma_review': SentimentSave, 'predict score': resultSave}\n",
    "    toFile = pd.DataFrame(data)\n",
    "    toFile.to_csv(\"./for_compare.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./1_Save_Tfidf_Model_Train/Summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gotj\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\model_config.py:76: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  config = yaml.load(yaml_string)\n"
     ]
    }
   ],
   "source": [
    "test = loadModel('./1_Save_Tfidf_Model_Train/Summary','./1_Save_Tfidf_Model_Train/Weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 9218, 32)          294976    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_3 (Spatial (None, 9218, 32)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50)                16600     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 311,627\n",
      "Trainable params: 311,627\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateModel(test,feat=X_test,target=Y_test,batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
