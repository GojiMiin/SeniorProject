{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, GRU, Dropout\n",
    "from tensorflow.keras.models import model_from_yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(filePath):\n",
    "    lemma_result = pd.read_csv(filePath)\n",
    "    print(lemma_result.shape)\n",
    "    return lemma_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beforeCreateModel(max_feat,dataset):\n",
    "    max_fatures = max_feat #จำนวนคำที่ใช้ใน model\n",
    "    tokenizer = Tokenizer(num_words=max_fatures, split=' ') \n",
    "    tokenizer.fit_on_texts(dataset['cleaned_review'].values)\n",
    "    X1 = tokenizer.texts_to_sequences(dataset['cleaned_review'].values)\n",
    "    \n",
    "    feat = pad_sequences(X1, padding='pre',maxlen=580) # ลองปรับ padding เป็น Post เผื่อค่าจะดีขึ้น\n",
    "    target = dataset['Label'].values\n",
    "    \n",
    "    return feat,target,tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModelLSTM(embed_dim,lstm_out,max_feat,input_length):\n",
    "    embed_dim = embed_dim\n",
    "    lstm_out = lstm_out\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_feat, embed_dim,input_length = input_length))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModelGRU(embed_dim,gru_out,max_feat,input_length):\n",
    "    embed_dim = embed_dim\n",
    "    gru_out = gru_out\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_feat, embed_dim,input_length = input_length))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(gru_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model,feat,target,validation_split,epochs,batch_size):\n",
    "    random.seed(10)\n",
    "    history = model.fit(feat, target,validation_split=validation_split, epochs = epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(model,feat,target,batch_size):\n",
    "    score,acc = model.evaluate(feat, target, verbose = 2, batch_size = batch_size)\n",
    "    print(\"score: %.2f\" % (score))\n",
    "    print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(model):\n",
    "    # serialize model to YAML\n",
    "    model_yaml = model.to_yaml()\n",
    "    with open(\"addTestTrainSize.yaml\", \"w\") as yaml_file:\n",
    "        yaml_file.write(model_yaml)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"addTestTrainSize.h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadModel(yamlPathName, h5PathName):\n",
    "    with open(yamlPathName+'.yaml', 'r') as yaml_file:\n",
    "        loaded_model_yaml = yaml_file.read()\n",
    "        loaded_model = model_from_yaml(loaded_model_yaml)\n",
    "\n",
    "        # load weights into new model\n",
    "        loaded_model.load_weights(h5PathName+'.h5')\n",
    "    \n",
    "    loaded_model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterAndShow(filter1, filter2, modelOutput, labelTest):\n",
    "    print(modelOutput)\n",
    "    test = []\n",
    "    test_y = []\n",
    "\n",
    "    for i in range(len(modelOutput)):\n",
    "        if(modelOutput[i] <filter1 or modelOutput[i]>filter2):\n",
    "            test.append(modelOutput[i])\n",
    "            test_y.append(labelTest[i])\n",
    "\n",
    "    test1 = np.array(test)\n",
    "    testy1 = np.array(test_y)\n",
    "    print(testy1)\n",
    "    print(test1.shape)\n",
    "    print(testy1.shape)\n",
    "    showConfusionMatrix(testy1, test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showConfusionMatrix(trueLabel,resultToShow):\n",
    "    labels = ['positive','negative']\n",
    "    cm = confusion_matrix(y_true=trueLabel , y_pred=resultToShow>0.5)\n",
    "    print(cm)\n",
    "    #fig = plt.figure()\n",
    "    #ax = fig.add_subplot(111)\n",
    "    #cax = ax.matshow(cm)\n",
    "    #plt.title('Confusion matrix of LSTM classifier')\n",
    "    #fig.colorbar(cax)\n",
    "    #ax.set_xticklabels(['']+labels)\n",
    "    #ax.set_yticklabels(['']+labels)\n",
    "    #plt.xlabel('Predicted')\n",
    "    #plt.ylabel('True')\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showWordWithCode(readIn, colName, dataToMap, tokenizer): #readIn = pd.read_csv ,colName = column name in string, dataToMap = list of sentiment\n",
    "    test = tokenizer.texts_to_sequences(readIn[colName].values)\n",
    "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items())) # map id to all word in dic\n",
    "    \n",
    "    def sequence_to_text(list_of_indices):\n",
    "        # Looking up words in dictionary\n",
    "        words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "        return(words)\n",
    "    \n",
    "    my_texts = list(map(sequence_to_text, dataToMap))\n",
    "    my_texts[13]\n",
    "    return my_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveSentimentAndResult(sentenceToSave, resultToSave):\n",
    "    sen_temp = \"\"\n",
    "    SentimentSave = []\n",
    "    for one_sentence in sentenceToSave:\n",
    "        for word in one_sentence:\n",
    "            if isinstance(word, str):\n",
    "                sen_temp = sen_temp + \" \" + word\n",
    "            \n",
    "        SentimentSave.append(sen_temp)\n",
    "        sen_temp = \"\"\n",
    "            \n",
    "    #make 1 Dim predict result\n",
    "    resultSave = []\n",
    "    for arr_result in resultToSave:\n",
    "        for result in arr_result:\n",
    "            #print(result)\n",
    "            resultSave.append(result)\n",
    "            \n",
    "    data = {'lemma_review': SentimentSave, 'predict score': resultSave}\n",
    "    toFile = pd.DataFrame(data)\n",
    "    toFile.to_csv(\"./for_compare.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "path = \"../Clean/lemma_result.csv\"\n",
    "x = readFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   0,    0,    0, ...,  406, 3108,  310],\n",
       "        [   0,    0,    0, ..., 2704,   18,  121],\n",
       "        [   0,    0,    0, ...,   10,    5,  332],\n",
       "        ...,\n",
       "        [   0,    0,    0, ...,    4,  645,  771],\n",
       "        [   0,    0,    0, ...,  964,  606,    1],\n",
       "        [   0,    0,    0, ...,   57,  101, 1004]]),\n",
       " array([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 0, 1, 1, 0, 0, 0, 0], dtype=int64))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat,target,tokenizer = beforeCreateModel(max_feat=7000,dataset=x)\n",
    "\n",
    "feat,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(feat,target, test_size = 0.2, train_size = 0.8, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 580, 150)          1050000   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 580, 150)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 200)               280800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 1,331,001\n",
      "Trainable params: 1,331,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = createModelLSTM(embed_dim=150,lstm_out=200,max_feat=7000,input_length=feat.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainModel(model,feat=X_train,target=Y_train,validation_split=0.2,epochs=50,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluateModel(model,feat=X_test,target=Y_test,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saveModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\model_config.py:76: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  config = yaml.load(yaml_string)\n"
     ]
    }
   ],
   "source": [
    "test = loadModel('./4_Save_model_cut_dummy_LSTM/addTestTrainSize','./4_Save_model_cut_dummy_LSTM/addTestTrainSize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 - 2s - loss: 1.6870 - accuracy: 0.6700\n",
      "score: 1.69\n",
      "acc: 0.67\n"
     ]
    }
   ],
   "source": [
    "evaluateModel(test,feat=X_test,target=Y_test,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02430036]\n",
      " [0.06550971]\n",
      " [0.00006281]\n",
      " [0.00007229]\n",
      " [0.99773383]\n",
      " [0.9999572 ]\n",
      " [0.9382391 ]\n",
      " [0.00060544]\n",
      " [0.00056729]\n",
      " [0.60899746]\n",
      " [0.94603825]\n",
      " [0.9981201 ]\n",
      " [0.00006075]\n",
      " [0.07131647]\n",
      " [0.01077259]\n",
      " [0.00031464]\n",
      " [0.9998952 ]\n",
      " [0.21666142]\n",
      " [0.13646378]\n",
      " [0.99333775]\n",
      " [0.99996173]\n",
      " [0.00010865]\n",
      " [0.00005511]\n",
      " [0.06085382]\n",
      " [0.00003648]\n",
      " [0.00593925]\n",
      " [0.01792044]\n",
      " [0.99984014]\n",
      " [0.01197829]\n",
      " [0.9998673 ]\n",
      " [0.00012733]\n",
      " [0.00004649]\n",
      " [0.34669885]\n",
      " [0.0424039 ]\n",
      " [0.02007715]\n",
      " [0.9781845 ]\n",
      " [0.2412503 ]\n",
      " [0.969636  ]\n",
      " [0.00029175]\n",
      " [0.99988294]\n",
      " [0.9656104 ]\n",
      " [0.56526744]\n",
      " [0.00030897]\n",
      " [0.00019033]\n",
      " [0.99966216]\n",
      " [0.6906669 ]\n",
      " [0.60334545]\n",
      " [0.9960824 ]\n",
      " [0.00057261]\n",
      " [0.99859434]\n",
      " [0.12571406]\n",
      " [0.00004569]\n",
      " [0.00068661]\n",
      " [0.00021745]\n",
      " [0.00010541]\n",
      " [0.9993191 ]\n",
      " [0.00001085]\n",
      " [0.97216153]\n",
      " [0.00015264]\n",
      " [0.00023316]\n",
      " [0.00007705]\n",
      " [0.9997602 ]\n",
      " [0.00137877]\n",
      " [0.5412207 ]\n",
      " [0.00092237]\n",
      " [0.8023472 ]\n",
      " [0.25606233]\n",
      " [0.68530506]\n",
      " [0.28611165]\n",
      " [0.00004721]\n",
      " [0.00007158]\n",
      " [0.6369487 ]\n",
      " [0.96717036]\n",
      " [0.00003369]\n",
      " [0.11105364]\n",
      " [0.02732045]\n",
      " [0.00016747]\n",
      " [0.00128237]\n",
      " [0.00001091]\n",
      " [0.9991698 ]\n",
      " [0.03217968]\n",
      " [0.99914885]\n",
      " [0.03954887]\n",
      " [0.5367927 ]\n",
      " [0.9999969 ]\n",
      " [0.99957484]\n",
      " [0.00002235]\n",
      " [0.9998424 ]\n",
      " [0.84101   ]\n",
      " [0.0001735 ]\n",
      " [0.98910004]\n",
      " [0.9989489 ]\n",
      " [0.00010966]\n",
      " [0.00318975]\n",
      " [0.96301764]\n",
      " [0.03099249]\n",
      " [0.00005923]\n",
      " [0.00015503]\n",
      " [0.00010501]\n",
      " [0.99671257]\n",
      " [0.00210568]\n",
      " [0.9998722 ]\n",
      " [0.00010225]\n",
      " [0.00005462]\n",
      " [0.00032056]\n",
      " [0.00025445]\n",
      " [0.01608308]\n",
      " [0.00001582]\n",
      " [0.0000278 ]\n",
      " [0.01388159]\n",
      " [0.9998437 ]\n",
      " [0.99822575]\n",
      " [0.01845812]\n",
      " [0.9975981 ]\n",
      " [0.9979996 ]\n",
      " [0.4985316 ]\n",
      " [0.9984371 ]\n",
      " [0.99911565]\n",
      " [0.00019614]\n",
      " [0.98293155]\n",
      " [0.9791018 ]\n",
      " [0.8977839 ]\n",
      " [0.0094039 ]\n",
      " [0.17633191]\n",
      " [0.00015671]\n",
      " [0.00621621]\n",
      " [0.0010531 ]\n",
      " [0.00119481]\n",
      " [0.02649556]\n",
      " [0.02133416]\n",
      " [0.16418113]\n",
      " [0.9999678 ]\n",
      " [0.6343144 ]\n",
      " [0.9987324 ]\n",
      " [0.77298546]\n",
      " [0.00205735]\n",
      " [0.99961406]\n",
      " [0.99983656]\n",
      " [0.99842   ]\n",
      " [0.00605008]\n",
      " [0.99773055]\n",
      " [0.01007142]\n",
      " [0.00016931]\n",
      " [0.00003464]\n",
      " [0.00457806]\n",
      " [0.6176608 ]\n",
      " [0.00382659]\n",
      " [0.0006975 ]\n",
      " [0.9994305 ]\n",
      " [0.99994373]\n",
      " [0.99992704]\n",
      " [0.04091877]\n",
      " [0.00058321]\n",
      " [0.99316025]\n",
      " [0.04389021]\n",
      " [0.00478359]\n",
      " [0.99810565]\n",
      " [0.59609646]\n",
      " [0.00017065]\n",
      " [0.9928134 ]\n",
      " [0.19913024]\n",
      " [0.9996805 ]\n",
      " [0.3197224 ]\n",
      " [0.9999485 ]\n",
      " [0.00005358]\n",
      " [0.00718099]\n",
      " [0.00323744]\n",
      " [0.5733737 ]\n",
      " [0.8187218 ]\n",
      " [0.99998975]\n",
      " [0.00000228]\n",
      " [0.00631537]\n",
      " [0.00023398]\n",
      " [0.00001246]\n",
      " [0.13059735]\n",
      " [0.98210967]\n",
      " [0.14202368]\n",
      " [0.01337509]\n",
      " [0.00014649]\n",
      " [0.9774473 ]\n",
      " [0.9999603 ]\n",
      " [0.01245917]\n",
      " [0.9949142 ]\n",
      " [0.00014929]\n",
      " [0.9984084 ]\n",
      " [0.00455478]\n",
      " [0.02408201]\n",
      " [0.99978775]\n",
      " [0.9415555 ]\n",
      " [0.99749774]\n",
      " [0.00025253]\n",
      " [0.96785384]\n",
      " [0.00305193]\n",
      " [0.15096217]\n",
      " [0.09587574]\n",
      " [0.0000427 ]\n",
      " [0.9999484 ]\n",
      " [0.54451734]\n",
      " [0.00001023]\n",
      " [0.03184431]]\n",
      "[1 1 0 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0\n",
      " 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 1 1 1 1\n",
      " 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
      " 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 0\n",
      " 0 1 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1\n",
      " 0 1 0 1 0 1 0 0]\n",
      "(193, 1)\n",
      "(193,)\n",
      "[[77 24]\n",
      " [40 52]]\n"
     ]
    }
   ],
   "source": [
    "filterAndShow(filter1=0.4, filter2=0.6, modelOutput=result, labelTest=Y_test) #LSTM with filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[78 26]\n",
      " [40 56]]\n"
     ]
    }
   ],
   "source": [
    "showConfusionMatrix(trueLabel=Y_test,resultToShow=result) #LSTM no filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[78, 26],\n",
       "       [40, 56]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true=Y_test, y_pred=result>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi=showWordWithCode(readIn=x, colName='cleaned_review', dataToMap=X_test, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveSentimentAndResult(hi,result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = createModelGRU(embed_dim=150,gru_out=200,max_feat=7000,input_length=feat.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainModel(model,feat=X_train,target=Y_train,validation_split=0.2,epochs=50,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateModel(model,feat=X_test,target=Y_test,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultGRU = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterAndShow(filter1=0.4, filter2=0.6, modelOutput=resultGRU, labelTest=Y_test) #GRU with filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showConfusionMatrix(trueLabel=Y_test,resultToShow=result) #GRU no filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveModel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2)\n"
     ]
    }
   ],
   "source": [
    "path = \"../Clean/lemma_resultscrape.csv\"\n",
    "x = readFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "testWR = loadModel('./5_Save_model_cut_dummy_GRU/addTestTrainSize','./5_Save_model_cut_dummy_GRU/addTestTrainSize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  0,   0,   0, ..., 227, 228,  72],\n",
       "        [  0,   0,   0, ..., 257,   6,   4],\n",
       "        [  0,   0,   0, ..., 284,   1,   1],\n",
       "        ...,\n",
       "        [  0,   0,   0, ..., 370, 371,  12],\n",
       "        [  0,   0,   0, ..., 401,  10,  72],\n",
       "        [  0,   0,   0, ...,  47,  48,  49]]),\n",
       " array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0], dtype=int64))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featWR,targetWR,tokenizer = beforeCreateModel(max_feat=7000,dataset=x)\n",
    "\n",
    "featWR,targetWR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featWR.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00115103],\n",
       "       [0.00017376],\n",
       "       [0.0006973 ],\n",
       "       [0.05847816],\n",
       "       [0.9454212 ],\n",
       "       [0.99995625],\n",
       "       [0.9846506 ],\n",
       "       [0.2361771 ],\n",
       "       [0.99977034],\n",
       "       [0.99397993]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultWR = test.predict(featWR)\n",
    "\n",
    "resultWR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 4]\n",
      " [4 1]]\n"
     ]
    }
   ],
   "source": [
    "showConfusionMatrix(trueLabel=targetWR,resultToShow=resultWR)\n",
    "#confusion_matrix(target[:,1] , result>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6(tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
