{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Freshness  Review\n",
      "0                                        Freshness,Review     NaN\n",
      "1       1,\" Manakamana doesn't answer any questions, y...     NaN\n",
      "2       1,\" Wilfully offensive and powered by a chest-...     NaN\n",
      "3       0, It would be difficult to imagine material m...     NaN\n",
      "4       0,\" Despite the gusto its star brings to the r...     NaN\n",
      "...                                                   ...     ...\n",
      "479996  0, Zemeckis seems unable to admit that the mot...     NaN\n",
      "479997  1,\" Movies like The Kids Are All Right -- beau...     NaN\n",
      "479998  0,\" Film-savvy audiences soon will catch onto ...     NaN\n",
      "479999                     1, An odd yet enjoyable film.      NaN\n",
      "480000  1,\" No other animation studio, even our belove...     NaN\n",
      "\n",
      "[480001 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "reSentence = []\n",
    "allLabel = []\n",
    "\n",
    "# bring in sentence from file\n",
    "data = pd.read_table(\"../Clean/raw_data/rotten_tomatoes_reviews.csv\",\n",
    "                    header=None,\n",
    "                    names=['Freshness', 'Review'])\n",
    "print(data)\n",
    "InFilter = data['Review'].astype('string')\n",
    "Label = data['Freshness'].astype('string')\n",
    "InFilter.drop(InFilter.head(0))\n",
    "\n",
    "for label in Label:\n",
    "    allLabel.append(label)\n",
    "\n",
    "#Encode Label\n",
    "#le = LabelEncoder()\n",
    "#EnLabel = le.fit_transform(allLabel)\n",
    "#print(EnLabel)\n",
    "#print(Label)\n",
    "\n",
    "#data = {'original label': Label, 'Encode label': EnLabel}\n",
    "#toFile = pd.DataFrame(data)\n",
    "#toFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slow move aimless movie distress drift young man\n"
     ]
    }
   ],
   "source": [
    "reSentence = []\n",
    "for sentence in InFilter:\n",
    "    lower_sentence = sentence.lower()\n",
    "    lemma_sentence_cleaned = []\n",
    "    clean = re.compile('<.*?>')\n",
    "    sentence_no_tag = re.sub(clean, '', lower_sentence)\n",
    "    #reSentence.append(sentence_no_tag)\n",
    "    \n",
    "    cleaned = []\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    token_sentence = tokenizer.tokenize(sentence_no_tag)\n",
    "    for w in token_sentence:\n",
    "        if not w in stopwords.words('English'):  # delete stopwords\n",
    "            cleaned.append(w)\n",
    "            #lemma_sentence_cleaned = \" \".join(cleaned)\n",
    "    lemma_word_cleaned = []\n",
    "    for word in cleaned:\n",
    "        lemma_word_cleaned.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "\n",
    "    lemma_sentence_cleaned = \" \".join(lemma_word_cleaned)\n",
    "    reSentence.append(lemma_sentence_cleaned)\n",
    "print(reSentence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'cleaned_review': reSentence, 'Label': Label}\n",
    "toFile = pd.DataFrame(data)\n",
    "toFile.to_csv(\"./lemma_result_imdb_data_text.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...  406 3108  310]\n",
      " [   0    0    0 ... 2704   18  121]\n",
      " [   0    0    0 ...   10    5  332]\n",
      " ...\n",
      " [   0    0    0 ...    4  645  771]\n",
      " [   0    0    0 ...  964  606    1]\n",
      " [   0    0    0 ...   57  101 1004]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import re\n",
    "\n",
    "lemma_result = pd.read_csv(\"../Clean/lemma_result.csv\") #Dataset\n",
    "\n",
    "max_fatures = 7000 #จำนวนคำที่ใช้ใน model\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ') \n",
    "tokenizer.fit_on_texts(lemma_result['cleaned_review'].values)\n",
    "X1 = tokenizer.texts_to_sequences(lemma_result['cleaned_review'].values)\n",
    "X1 = pad_sequences(X1, padding='pre') \n",
    "\n",
    "print(X1)\n",
    "app = []\n",
    "\n",
    "for i in X1:\n",
    "    app.append(i)\n",
    "    \n",
    "print(app)\n",
    "\n",
    "#forPrint = {'Token': X1}\n",
    "#toFile = pd.DataFrame(forPrint)\n",
    "#toFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6(tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
