{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.layers import LayerNormalization,BatchNormalization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, GRU, RNN, Dropout\n",
    "from tensorflow.keras.models import model_from_yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(filePath):\n",
    "    lemma_result = pd.read_csv(filePath)\n",
    "    print(lemma_result.shape)\n",
    "    return lemma_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkLength(max_feat, file):\n",
    "    leng = 0\n",
    "    sentence = []\n",
    "    allData = file['cleaned_review']\n",
    "\n",
    "    max_fatures = max_feat\n",
    "    tokenizer = Tokenizer(num_words=max_fatures, split=' ') \n",
    "    tokenizer.fit_on_texts(allData.values)\n",
    "    X1 = tokenizer.texts_to_sequences(allData.values)\n",
    "    \n",
    "    for i in X1:\n",
    "        if len(i)>leng:\n",
    "            leng = len(i)\n",
    "            sentence = i\n",
    "    print(leng)\n",
    "    print(sentence)\n",
    "    return leng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beforeCreateModel(max_feat,dataset,max_length):\n",
    "    max_fatures = max_feat #จำนวนคำที่ใช้ใน model\n",
    "    tokenizer = Tokenizer(num_words=max_fatures, split=' ') \n",
    "    tokenizer.fit_on_texts(dataset['cleaned_review'].values)\n",
    "    X1 = tokenizer.texts_to_sequences(dataset['cleaned_review'].values)\n",
    "    print(len(tokenizer.word_index))\n",
    "    feat = pad_sequences(X1, padding='pre',maxlen=max_length) # 505 = max_length in sentence\n",
    "    target = dataset['Label'].values\n",
    "\n",
    "    return feat,target,tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModelLSTM(embed_dim,lstm_out,max_feat,input_length): #(MAX_FEAT=5000)\n",
    "    embed_dim = embed_dim\n",
    "    lstm_out = lstm_out\n",
    "    model = Sequential() #Create Model\n",
    "    model.add(Embedding(input_dim = max_feat ,output_dim = embed_dim ,input_length = input_length)) #Input Layer\n",
    "    model.add(RNN(lstm_out)) #1st hidden Layer\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1,activation='sigmoid')) # Output Layer\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModelGRU(embed_dim,gru_out,max_feat,input_length):\n",
    "    embed_dim = embed_dim\n",
    "    gru_out = gru_out\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_feat, embed_dim,input_length = input_length)) \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(gru_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model,feat,target,validation_split,epochs,batch_size):\n",
    "    random.seed(10)\n",
    "    history = model.fit(feat, target,validation_split=validation_split, epochs = epochs, batch_size=batch_size)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(model,feat,target,batch_size):\n",
    "    score,acc = model.evaluate(feat, target, verbose = 2, batch_size = batch_size)\n",
    "    print(\"score: %.2f\" % (score))\n",
    "    print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLoss(history):\n",
    "    loss_values = history.history['loss']\n",
    "    val_loss_values = history.history['val_loss']\n",
    "    epochs = range(1, len(loss_values)+1)\n",
    "\n",
    "    plt.plot(epochs, loss_values, label='Training Loss')\n",
    "    plt.plot(epochs, val_loss_values, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(model):\n",
    "    # serialize model to YAML\n",
    "    model_yaml = model.to_yaml()\n",
    "    with open(\"Summary.yaml\", \"w\") as yaml_file:\n",
    "        yaml_file.write(model_yaml)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"Weights.h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadModel(yamlPathName, h5PathName):\n",
    "    with open(yamlPathName+'.yaml', 'r') as yaml_file:\n",
    "        print(yamlPathName)\n",
    "        loaded_model_yaml = yaml_file.read()\n",
    "        loaded_model = model_from_yaml(loaded_model_yaml)\n",
    "        loaded_model.load_weights(h5PathName+'.h5')\n",
    "    loaded_model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterAndShow(filter1, filter2, modelOutput, labelTest):\n",
    "    print(modelOutput)\n",
    "    test = []\n",
    "    test_y = []\n",
    "\n",
    "    for i in range(len(modelOutput)):\n",
    "        if(modelOutput[i] <filter1 or modelOutput[i]>filter2):\n",
    "            test.append(modelOutput[i])\n",
    "            test_y.append(labelTest[i])\n",
    "\n",
    "    test1 = np.array(test)\n",
    "    testy1 = np.array(test_y)\n",
    "    print(testy1)\n",
    "    print(test1.shape)\n",
    "    print(testy1.shape)\n",
    "    showConfusionMatrix(testy1, test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showConfusionMatrix(trueLabel,resultToShow):\n",
    "    labels = ['positive','negative']\n",
    "    cm = confusion_matrix(y_true=trueLabel , y_pred=resultToShow>0.5)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showWordWithCode(dataToMap, tokenizer): #dataToMap = list of sentiment\n",
    "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items())) # map id to all word in dic\n",
    "    print(reverse_word_map)\n",
    "    \n",
    "    def sequence_to_text(list_of_indices):\n",
    "        # Looking up words in dictionary\n",
    "        words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "        return(words)\n",
    "    \n",
    "    my_texts = list(map(sequence_to_text, dataToMap))\n",
    "    #my_texts\n",
    "    return reverse_word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveSentimentAndResult(sentenceToSave, resultToSave):\n",
    "    sen_temp = \"\"\n",
    "    SentimentSave = []\n",
    "    for one_sentence in sentenceToSave:\n",
    "        for word in one_sentence:\n",
    "            if isinstance(word, str):\n",
    "                sen_temp = sen_temp + \" \" + word\n",
    "            \n",
    "        SentimentSave.append(sen_temp)\n",
    "        sen_temp = \"\"\n",
    "            \n",
    "    #make 1 Dim predict result\n",
    "    resultSave = []\n",
    "    for arr_result in resultToSave:\n",
    "        for result in arr_result:\n",
    "            #print(result)\n",
    "            resultSave.append(result)\n",
    "            \n",
    "    data = {'lemma_review': SentimentSave, 'predict score': resultSave}\n",
    "    toFile = pd.DataFrame(data)\n",
    "    toFile.to_csv(\"./for_compare.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n"
     ]
    }
   ],
   "source": [
    "path = \"../Clean/lemma_allresult.csv\"\n",
    "x = readFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "853\n",
      "[569, 935, 28, 155, 21, 1153, 1780, 3, 48, 1, 1020, 352, 2883, 261, 179, 29, 6126, 7109, 3174, 7996, 211, 913, 1419, 3479, 1427, 4161, 397, 1018, 941, 289, 2916, 2014, 245, 1398, 2950, 1087, 7, 529, 3581, 46, 1000, 200, 1, 150, 648, 113, 1043, 3785, 159, 1576, 2636, 698, 41, 122, 1231, 3581, 52, 1, 1153, 1780, 5, 3581, 60, 203, 529, 1153, 1780, 44, 1096, 121, 99, 236, 33, 3581, 4116, 1359, 23, 33, 1153, 1780, 753, 152, 1153, 1780, 61, 27, 155, 190, 828, 3454, 3581, 61, 292, 1719, 2903, 49, 2311, 31, 302, 292, 43, 272, 576, 17, 895, 781, 307, 343, 1457, 5, 1696, 3199, 27, 1353, 13, 12, 172, 165, 968, 47, 2000, 23, 2311, 3454, 19, 125, 984, 18, 933, 904, 38, 301, 5224, 144, 301, 2444, 131, 6015, 2497, 52, 1, 624, 5919, 31, 3, 152, 15, 1153, 1780, 52, 3581, 1153, 1780, 110, 2561, 2017, 144, 302, 4, 362, 767, 1431, 79, 45, 33, 541, 161, 651, 2321, 902, 2676, 29, 8781, 3230, 31, 29, 453, 155, 93, 394, 902, 1973, 393, 2817, 25, 1, 29, 1153, 48, 39, 39, 349, 1153, 254, 986, 31, 1019, 292, 155, 93, 87, 70, 20, 1, 161, 196, 609, 2143, 576, 495, 1048, 796, 17, 529, 46, 33, 3, 2252, 83, 209, 828, 3454, 3581, 46, 1599, 766, 907, 278, 20, 8, 179, 1418, 3484, 185, 1090, 1576, 82, 400, 1153, 1780, 253, 3926, 144, 984, 367, 1153, 1780, 986, 79, 3164, 1741, 4920, 56, 1038, 72, 1153, 788, 22, 1153, 107, 1066, 1114, 1741, 1114, 14, 1281, 112, 237, 1153, 986, 1387, 2156, 1528, 1153, 1780, 1171, 1038, 175, 3906, 3581, 19, 84, 907, 119, 468, 1148, 53, 2017, 7536, 2462, 1312, 3901, 2455, 3706, 32, 902, 624, 3785, 3199, 3454, 435, 4229, 3399, 138, 153, 1148, 10, 3581, 3345, 62, 1227, 359, 10, 907, 119, 175, 1038, 856, 105, 3, 1755, 342, 8775, 695, 344, 193, 1597, 3454, 8449, 1153, 1780, 180, 1528, 289, 28, 155, 87, 5976, 53, 2046, 453, 588, 1337, 155, 1233, 53, 714, 6, 2642, 425, 226, 1672, 53, 2046, 388, 2100, 53, 26, 3180, 651, 2676, 329, 2247, 453, 2046, 7730, 342, 7012, 695, 4, 858, 4920, 1153, 1780, 31, 1780, 193, 7908, 122, 2428, 128, 2182, 9, 5976, 241, 4920, 1038, 10, 41, 995, 740, 4870, 2374, 3596, 5976, 108, 234, 44, 22, 5976, 61, 234, 1409, 1309, 4870, 3779, 34, 1978, 118, 13, 438, 23, 5, 342, 573, 53, 453, 1076, 5277, 258, 2744, 102, 320, 45, 2767, 1427, 4161, 149, 1, 2694, 21, 581, 4920, 236, 342, 31, 4797, 367, 1, 94, 53, 12, 96, 117, 1318, 27, 4015, 4920, 23, 1177, 10, 79, 794, 27, 41, 456, 301, 49, 1153, 1780, 236, 927, 93, 1930, 456, 230, 357, 367, 301, 382, 7035, 301, 380, 495, 7699, 4373, 1153, 1780, 986, 30, 93, 372, 3684, 5516, 1514, 120, 874, 94, 82, 10, 38, 1037, 2676, 198, 456, 2902, 5204, 193, 1597, 5976, 715, 8577, 1153, 1780, 986, 193, 5976, 1672, 30, 94, 372, 27, 388, 1938, 27, 5, 5976, 1279, 1119, 383, 4291, 1153, 1780, 678, 73, 631, 57, 3581, 3, 71, 7, 924, 27, 72, 3581, 52, 1, 631, 57, 28, 631, 57, 907, 119, 8182, 264, 97, 2258, 1043, 1898, 631, 57, 3581, 48, 631, 57, 51, 121, 1, 714, 3, 515, 357, 631, 57, 1906, 445, 225, 2000, 7, 272, 4291, 907, 119, 286, 113, 150, 73, 8, 45, 1, 4291, 631, 57, 1906, 4291, 4920, 1153, 1780, 796, 4769, 3, 663, 57, 1, 382, 155, 472, 28, 225, 290, 651, 46, 10, 4291, 653, 10, 805, 4920, 1484, 1153, 1780, 534, 7010, 4920, 59, 4960, 2504, 1153, 59, 4920, 225, 1388, 9835, 3485, 1780, 225, 290, 489, 137, 1, 3619, 440, 28, 219, 678, 199, 219, 142, 464, 780, 365, 581, 1153, 107, 1811, 342, 2462, 137, 94, 1153, 307, 7, 2941, 263, 1023, 3, 663, 331, 1, 382, 1153, 604, 79, 2014, 780, 73, 4920, 59, 115, 308, 1510, 55, 9, 5516, 61, 342, 780, 6, 1951, 7, 2171, 2000, 1264, 185, 1153, 1780, 661, 3581, 22, 27, 240, 5, 27, 1153, 1780, 56, 594, 253, 601, 2082, 4085, 56, 45, 27, 170, 1153, 1780, 236, 222, 10, 21, 303, 171, 121, 304, 6613, 1153, 1780, 76, 3, 663, 84, 1, 1020, 45, 324, 1, 896, 185, 11, 1, 248, 8, 222, 1597, 324, 3581, 907, 64, 169, 173, 188, 10, 35, 1699, 1719, 594, 253, 601, 2082, 8, 1, 32, 141, 362, 5936, 1153, 1780, 159, 240, 5, 27, 398, 1199, 152, 45, 27, 5, 52, 1073, 398, 92, 2949, 3049, 47, 32, 8, 4633, 222, 90, 1207, 1409, 45, 1350, 1, 173, 6, 390, 4572, 465, 789, 1498, 390, 144, 157, 176, 920, 161, 581, 3454, 186, 5, 8769, 68, 1729, 248, 1, 570, 264, 2258, 148, 186, 478, 1153, 1780, 52, 1, 3581, 907, 478, 3, 71, 17, 907, 1097, 1733, 3581, 52, 1, 1153, 1780, 121, 236, 1153, 1780, 1, 415, 398, 94, 3581, 1, 1719, 933, 907, 661, 4, 1889, 15, 52, 971, 416, 721]\n"
     ]
    }
   ],
   "source": [
    "max_length = checkLength(MAX_FEATURE,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44402\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[   0,    0,    0, ...,  387, 3283,  334],\n",
       "        [   0,    0,    0, ..., 1878,   19,  125],\n",
       "        [   0,    0,    0, ...,   12,    6,  275],\n",
       "        ...,\n",
       "        [   0,    0,    0, ...,    2,  252,   73],\n",
       "        [   0,    0,    0, ...,  343,    2,    9],\n",
       "        [   0,    0,    0, ..., 4874,  846, 1661]]),\n",
       " array([1, 1, 1, ..., 0, 0, 1], dtype=int64))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat,target,tokenizer = beforeCreateModel(max_feat=MAX_FEATURE,dataset=x,max_length=max_length)\n",
    "\n",
    "feat,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(feat,target, test_size = 0.2, train_size = 0.8, random_state = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('`cell` should have a `call` method. The RNN was passed:', 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-a8ebde1f2c30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlstm_out\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreateModelLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_feat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_FEATURE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input Shape is \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-4e660a0186b0>\u001b[0m in \u001b[0;36mcreateModelLSTM\u001b[1;34m(embed_dim, lstm_out, max_feat, input_length)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Create Model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_feat\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0moutput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0minput_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Input Layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#1st hidden Layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Output Layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, cell, return_sequences, return_state, go_backwards, stateful, unroll, time_major, **kwargs)\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'call'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m       raise ValueError('`cell` should have a `call` method. '\n\u001b[1;32m--> 393\u001b[1;33m                        'The RNN was passed:', cell)\n\u001b[0m\u001b[0;32m    394\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'state_size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m       raise ValueError('The RNN cell should have '\n",
      "\u001b[1;31mValueError\u001b[0m: ('`cell` should have a `call` method. The RNN was passed:', 32)"
     ]
    }
   ],
   "source": [
    "embed_dim=128\n",
    "lstm_out=32\n",
    "\n",
    "model = createModelLSTM(embed_dim=embed_dim,lstm_out=lstm_out,max_feat=MAX_FEATURE,input_length=feat.shape[1])\n",
    "print(\"Input Shape is \" + str(feat.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#history = trainModel(model,feat=X_train,target=Y_train,validation_split=0.2,epochs=15,batch_size=32)\n",
    "random.seed(10)\n",
    "history = model.fit(X_train, \n",
    "                    Y_train,\n",
    "                    validation_data=(X_test, Y_test), \n",
    "                    epochs = 10, \n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluateModel(model,feat=X_test,target=Y_test,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotLoss(history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = loadModel('./main_1_LSTM/Summary','./main_1_LSTM/Weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showConfusionMatrix(trueLabel=Y_test,resultToShow=result) #LSTM no filter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6(tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
