{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0\n",
      " 0 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1\n",
      " 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0\n",
      " 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1\n",
      " 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0\n",
      " 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 0 1\n",
      " 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1\n",
      " 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0\n",
      " 0 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1\n",
      " 1 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0 0 1 1 0 1\n",
      " 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 0\n",
      " 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 0 0 1 0\n",
      " 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1\n",
      " 1 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0\n",
      " 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0\n",
      " 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1\n",
      " 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1 0\n",
      " 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0\n",
      " 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 0 1\n",
      " 0 1 0 1 0 1 1 1 0 1 1 0 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0\n",
      " 0 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1\n",
      " 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1\n",
      " 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1\n",
      " 1 1 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0\n",
      " 1 0 0 1 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1\n",
      " 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0\n",
      " 0]\n",
      "0      positive\n",
      "1      positive\n",
      "2      positive\n",
      "3      negative\n",
      "4      positive\n",
      "         ...   \n",
      "995    positive\n",
      "996    negative\n",
      "997    negative\n",
      "998    negative\n",
      "999    negative\n",
      "Name: sentiment, Length: 1000, dtype: string\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "reSentence = []\n",
    "allLabel = []\n",
    "\n",
    "# bring in sentence from file\n",
    "readIn = pd.read_csv(\"../Clean/IMDB Dataset.csv\")\n",
    "sample = readIn.head(1000)\n",
    "InFilter = sample['review'].astype('string')\n",
    "Label = sample['sentiment'].astype('string')\n",
    "InFilter.drop(InFilter.head(0))\n",
    "\n",
    "for label in Label:\n",
    "    allLabel.append(label)\n",
    "\n",
    "#Encode Label\n",
    "#le = LabelEncoder()\n",
    "#EnLabel = le.fit_transform(allLabel)\n",
    "#print(EnLabel)\n",
    "#print(Label)\n",
    "\n",
    "#data = {'original label': Label, 'Encode label': EnLabel}\n",
    "#toFile = pd.DataFrame(data)\n",
    "#toFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one reviewers mention watch 1 oz episode hook right exactly happen first thing strike oz brutality unflinching scenes violence set right word go trust show faint hearted timid show pull punch regard drug sex violence hardcore classic use word call oz nickname give oswald maximum security state penitentary focus mainly emerald city experimental section prison cells glass front face inwards privacy high agenda em city home many aryans muslims gangstas latinos christians italians irish scuffle death star dodgy deal shady agreements never far away would say main appeal show due fact go show dare forget pretty picture paint mainstream audiences forget charm forget romance oz mess around first episode ever saw strike nasty surreal say ready watch develop taste oz get accustom high level graphic violence violence injustice crook guard sell nickel inmates kill order get away well mannered middle class inmates turn prison bitch due lack street skills prison experience watch oz may become comfortable uncomfortable view thats get touch darker side\n"
     ]
    }
   ],
   "source": [
    "reSentence = []\n",
    "for sentence in InFilter:\n",
    "    lower_sentence = sentence.lower()\n",
    "    lemma_sentence_cleaned = []\n",
    "    clean = re.compile('<.*?>')\n",
    "    sentence_no_tag = re.sub(clean, '', lower_sentence)\n",
    "    #reSentence.append(sentence_no_tag)\n",
    "    \n",
    "    cleaned = []\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    token_sentence = tokenizer.tokenize(sentence_no_tag)\n",
    "    for w in token_sentence:\n",
    "        if not w in stopwords.words('English'):  # delete stopwords\n",
    "            cleaned.append(w)\n",
    "            #lemma_sentence_cleaned = \" \".join(cleaned)\n",
    "    lemma_word_cleaned = []\n",
    "    for word in cleaned:\n",
    "        lemma_word_cleaned.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "\n",
    "    lemma_sentence_cleaned = \" \".join(lemma_word_cleaned)\n",
    "    reSentence.append(lemma_sentence_cleaned)\n",
    "print(reSentence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...  406 3108  310]\n",
      " [   0    0    0 ... 2704   18  121]\n",
      " [   0    0    0 ...   10    5  332]\n",
      " ...\n",
      " [   0    0    0 ...    4  645  771]\n",
      " [   0    0    0 ...  964  606    1]\n",
      " [   0    0    0 ...   57  101 1004]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import re\n",
    "\n",
    "lemma_result = pd.read_csv(\"../Clean/lemma_result.csv\") #Dataset\n",
    "\n",
    "max_fatures = 7000 #จำนวนคำที่ใช้ใน model\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ') \n",
    "tokenizer.fit_on_texts(lemma_result['cleaned_review'].values)\n",
    "X1 = tokenizer.texts_to_sequences(lemma_result['cleaned_review'].values)\n",
    "X1 = pad_sequences(X1, padding='pre') \n",
    "\n",
    "print(X1)\n",
    "app = []\n",
    "\n",
    "for i in X1:\n",
    "    app.append(i)\n",
    "    \n",
    "print(app)\n",
    "\n",
    "#forPrint = {'Token': X1}\n",
    "#toFile = pd.DataFrame(forPrint)\n",
    "#toFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6(tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
