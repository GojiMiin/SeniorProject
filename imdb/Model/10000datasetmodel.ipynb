{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import random\n",
    "import re\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.layers import LayerNormalization,BatchNormalization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, GRU, Dropout\n",
    "from tensorflow.keras.models import model_from_yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(filePath):\n",
    "    lemma_result = pd.read_csv(filePath)\n",
    "    print(lemma_result.shape)\n",
    "    return lemma_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkLength(max_feat, file):\n",
    "    leng = 0\n",
    "    sentence = []\n",
    "    allData = file['cleaned_review']\n",
    "\n",
    "    max_fatures = max_feat\n",
    "    tokenizer = Tokenizer(num_words=max_fatures, split=' ') \n",
    "    tokenizer.fit_on_texts(allData.values)\n",
    "    X1 = tokenizer.texts_to_sequences(allData.values)\n",
    "    \n",
    "    for i in X1:\n",
    "        if len(i)>leng:\n",
    "            leng = len(i)\n",
    "            sentence = i\n",
    "    print(leng)\n",
    "    print(sentence)\n",
    "    return leng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beforeCreateModel(max_feat,dataset,max_length):\n",
    "    max_fatures = max_feat #จำนวนคำที่ใช้ใน model\n",
    "    tokenizer = Tokenizer(num_words=max_fatures, split=' ') \n",
    "    tokenizer.fit_on_texts(dataset['cleaned_review'].values)\n",
    "    X1 = tokenizer.texts_to_sequences(dataset['cleaned_review'].values)\n",
    "    print(len(tokenizer.word_index))\n",
    "    feat = pad_sequences(X1, padding='pre',maxlen=max_length) # 505 = max_length in sentence\n",
    "    target = dataset['Label'].values\n",
    "\n",
    "    return feat,target,tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModelLSTM(embed_dim,lstm_out,max_feat,input_length):\n",
    "    embed_dim = embed_dim\n",
    "    lstm_out = lstm_out\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = max_feat ,output_dim = embed_dim ,input_length = input_length)) #input_dim = max_feat, #output_dim = embed_dim\n",
    "    model.add(LSTM(lstm_out, dropout=0.5, recurrent_dropout=0.5))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModelGRU(embed_dim,gru_out,max_feat,input_length):\n",
    "    embed_dim = embed_dim\n",
    "    gru_out = gru_out\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_feat, embed_dim,input_length = input_length)) \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(gru_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model,feat,target,validation_split,epochs,batch_size):\n",
    "    random.seed(10)\n",
    "    history = model.fit(feat, target,validation_split=validation_split, epochs = epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(model,feat,target,batch_size):\n",
    "    score,acc = model.evaluate(feat, target, verbose = 2, batch_size = batch_size)\n",
    "    print(\"score: %.2f\" % (score))\n",
    "    print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(model):\n",
    "    # serialize model to YAML\n",
    "    model_yaml = model.to_yaml()\n",
    "    with open(\"addTestTrainSize.yaml\", \"w\") as yaml_file:\n",
    "        yaml_file.write(model_yaml)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"addTestTrainSize.h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadModel(yamlPathName, h5PathName):\n",
    "    with open(yamlPathName+'.yaml', 'r') as yaml_file:\n",
    "        print(yamlPathName)\n",
    "        loaded_model_yaml = yaml_file.read()\n",
    "        loaded_model = model_from_yaml(loaded_model_yaml)\n",
    "\n",
    "        # load weights into new model\n",
    "        loaded_model.load_weights(h5PathName+'.h5')\n",
    "    \n",
    "    loaded_model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterAndShow(filter1, filter2, modelOutput, labelTest):\n",
    "    print(modelOutput)\n",
    "    test = []\n",
    "    test_y = []\n",
    "\n",
    "    for i in range(len(modelOutput)):\n",
    "        if(modelOutput[i] <filter1 or modelOutput[i]>filter2):\n",
    "            test.append(modelOutput[i])\n",
    "            test_y.append(labelTest[i])\n",
    "\n",
    "    test1 = np.array(test)\n",
    "    testy1 = np.array(test_y)\n",
    "    print(testy1)\n",
    "    print(test1.shape)\n",
    "    print(testy1.shape)\n",
    "    showConfusionMatrix(testy1, test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showConfusionMatrix(trueLabel,resultToShow):\n",
    "    labels = ['positive','negative']\n",
    "    cm = confusion_matrix(y_true=trueLabel , y_pred=resultToShow>0.5)\n",
    "    print(cm)\n",
    "    #fig = plt.figure()\n",
    "    #ax = fig.add_subplot(111)\n",
    "    #cax = ax.matshow(cm)\n",
    "    #plt.title('Confusion matrix of LSTM classifier')\n",
    "    #fig.colorbar(cax)\n",
    "    #ax.set_xticklabels(['']+labels)\n",
    "    #ax.set_yticklabels(['']+labels)\n",
    "    #plt.xlabel('Predicted')\n",
    "    #plt.ylabel('True')\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showWordWithCode(dataToMap, tokenizer): #dataToMap = list of sentiment\n",
    "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items())) # map id to all word in dic\n",
    "    \n",
    "    def sequence_to_text(list_of_indices):\n",
    "        # Looking up words in dictionary\n",
    "        words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "        return(words)\n",
    "    \n",
    "    my_texts = list(map(sequence_to_text, dataToMap))\n",
    "    my_texts\n",
    "    return my_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveSentimentAndResult(sentenceToSave, resultToSave):\n",
    "    sen_temp = \"\"\n",
    "    SentimentSave = []\n",
    "    for one_sentence in sentenceToSave:\n",
    "        for word in one_sentence:\n",
    "            if isinstance(word, str):\n",
    "                sen_temp = sen_temp + \" \" + word\n",
    "            \n",
    "        SentimentSave.append(sen_temp)\n",
    "        sen_temp = \"\"\n",
    "            \n",
    "    #make 1 Dim predict result\n",
    "    resultSave = []\n",
    "    for arr_result in resultToSave:\n",
    "        for result in arr_result:\n",
    "            #print(result)\n",
    "            resultSave.append(result)\n",
    "            \n",
    "    data = {'lemma_review': SentimentSave, 'predict score': resultSave}\n",
    "    toFile = pd.DataFrame(data)\n",
    "    toFile.to_csv(\"./for_compare.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../Clean/lemma10000_master_result.csv\"\n",
    "x = readFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = checkLength(10000,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat,target,tokenizer = beforeCreateModel(max_feat=10000,dataset=x,max_length=max_length)\n",
    "\n",
    "feat,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(feat,target, test_size = 0.2, train_size = 0.8, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = createModelLSTM(embed_dim=128,lstm_out=128,max_feat=10000,input_length=feat.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainModel(model,feat=X_train,target=Y_train,validation_split=0.2,epochs=15,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluateModel(model,feat=X_test,target=Y_test,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = loadModel('./11_Save_model_10000_data_LSTM/addTestTrainSize','./11_Save_model_10000_data_LSTM/addTestTrainSize')\n",
    "\n",
    "#test = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateModel(test,feat=X_test,target=Y_test,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filterAndShow(filter1=0.4, filter2=0.6, modelOutput=result, labelTest=Y_test) #LSTM with filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showConfusionMatrix(trueLabel=Y_test,resultToShow=result) #LSTM no filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_true=Y_test, y_pred=result>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi=showWordWithCode(dataToMap=X_test, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveSentimentAndResult(hi,result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = createModelGRU(embed_dim=150,gru_out=200,max_feat=7000,input_length=feat.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainModel(model,feat=X_train,target=Y_train,validation_split=0.2,epochs=50,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateModel(model,feat=X_test,target=Y_test,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultGRU = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterAndShow(filter1=0.4, filter2=0.6, modelOutput=resultGRU, labelTest=Y_test) #GRU with filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showConfusionMatrix(trueLabel=Y_test,resultToShow=result) #GRU no filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveModel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one reviewers mention watch 1 oz episode hook ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production film technique una...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>think wonderful way spend time hot summer week...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically family little boy jake think zombie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei love time money visually stun fi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>nothing sacred ask ernie fosselius days everyb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>hat hate self aware pretentious inanity masque...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>usually try professional constructive criticiz...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>like go see film history class something like ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>like zoology textbook give depiction animals a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_review  Label\n",
       "0    one reviewers mention watch 1 oz episode hook ...      1\n",
       "1    wonderful little production film technique una...      1\n",
       "2    think wonderful way spend time hot summer week...      1\n",
       "3    basically family little boy jake think zombie ...      0\n",
       "4    petter mattei love time money visually stun fi...      1\n",
       "..                                                 ...    ...\n",
       "995  nothing sacred ask ernie fosselius days everyb...      1\n",
       "996  hat hate self aware pretentious inanity masque...      0\n",
       "997  usually try professional constructive criticiz...      0\n",
       "998  like go see film history class something like ...      0\n",
       "999  like zoology textbook give depiction animals a...      0\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../Clean/lemma_result.csv\"\n",
    "x = readFile(path)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./12_Save_model_10000_masterimdb_LSTM/addTestTrainSize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\model_config.py:76: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  config = yaml.load(yaml_string)\n"
     ]
    }
   ],
   "source": [
    "testWR = loadModel('./12_Save_model_10000_masterimdb_LSTM/addTestTrainSize','./12_Save_model_10000_masterimdb_LSTM/addTestTrainSize')\n",
    "\n",
    "#testWR = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "603\n",
      "[14, 1939, 656, 49, 156, 68, 128, 36, 37, 183, 1939, 424, 4164, 4722, 1929, 42, 1145, 46, 6, 1939, 49, 3410, 38, 751, 183, 797, 193, 30, 396, 7406, 2, 1547, 6, 2007, 873, 55, 66, 29, 14, 780, 327, 12, 1939, 158, 4436, 232, 7406, 59, 19, 751, 2344, 889, 2, 29, 46, 6, 120, 183, 267, 133, 24, 1939, 5473, 1, 44, 730, 2887, 3476, 133, 556, 1733, 5432, 1, 118, 221, 42, 779, 133, 3127, 226, 302, 4370, 4371, 226, 968, 300, 3083, 2334, 916, 1940, 3626, 2722, 15, 6, 7407, 133, 155, 178, 89, 222, 26, 328, 4370, 4371, 226, 699, 5489, 41, 226, 41, 226, 7, 330, 99, 4370, 4371, 4, 1478, 3591, 5489, 2512, 4, 1557, 11, 1, 68, 128, 99, 4, 140, 13, 223, 1546, 2, 10, 391, 37, 4370, 4371, 140, 746, 1939, 1233, 11, 226, 164, 19, 14, 4088, 42, 99, 6, 3127, 226, 173, 1018, 2, 521, 5, 226, 277, 4, 2, 631, 12, 81, 420, 843, 631, 2, 20, 902, 12, 81, 113, 27, 81, 226, 2, 15, 7345, 740, 1784, 1172, 1079, 91, 1300, 89, 24, 6, 3127, 268, 18, 1503, 2180, 3626, 2722, 873, 4973, 4078, 6460, 64, 364, 42, 424, 64, 364, 49, 1929, 4973, 4078, 268, 776, 3127, 91, 133, 155, 13, 577, 24, 37, 858, 3083, 6400, 1940, 933, 966, 349, 155, 3, 26, 3083, 1610, 1010, 7408, 196, 7409, 158, 113, 9, 2, 3607, 42, 2749, 3083, 57, 2334, 1940, 200, 57, 196, 1364, 1882, 158, 113, 4271, 48, 364, 15, 320, 57, 1882, 5057, 2306, 78, 1050, 1241, 1940, 517, 7410, 1462, 1882, 4271, 76, 5490, 260, 6, 3083, 684, 1940, 3407, 3003, 916, 2638, 7237, 14, 1940, 2334, 3047, 68, 128, 4372, 916, 2638, 18, 469, 5436, 3083, 31, 4150, 196, 7409, 15, 31, 2961, 7411, 53, 58, 1279, 2007, 873, 55, 64, 364, 398, 54, 417, 2018, 3180, 7412, 7413, 1867, 106, 6, 472, 1940, 5490, 178, 4372, 282, 171, 1244, 205, 109, 309, 1940, 5490, 7253, 97, 27, 1495, 107, 2007, 873, 55, 684, 7408, 19, 581, 158, 44, 4707, 2927, 224, 1244, 5702, 1940, 155, 560, 684, 4225, 387, 1244, 761, 276, 4372, 1212, 1867, 106, 872, 178, 3180, 7412, 1438, 50, 4220, 268, 68, 145, 133, 328, 1472, 291, 730, 349, 455, 8, 24, 6, 3626, 2722, 528, 18, 42, 701, 483, 60, 38, 2, 1438, 50, 37, 653, 3626, 2722, 201, 2361, 7414, 15, 262, 2, 30, 100, 1438, 7, 5, 201, 207, 4, 381, 2361, 7257, 6903, 1958, 3626, 2722, 3878, 2886, 392, 310, 3623, 12, 336, 2361, 7414, 2864, 3623, 89, 2361, 226, 6384, 326, 310, 130, 3623, 7415, 53, 139, 15, 121, 53, 1438, 201, 15, 121, 41, 5, 414, 230, 226, 3623, 91, 435, 14, 7416, 3843, 629, 24, 574, 97, 474, 13, 56, 7417, 36, 91, 133, 1939, 7, 42, 447, 43, 10, 1, 168, 109, 50, 10, 1, 1601, 18, 127, 56, 474, 146, 20, 173, 177, 5, 28, 10, 241, 168, 3503, 14, 656, 1676, 1, 12, 17, 173, 2205, 36, 376, 42, 865, 1390, 1003, 379, 68, 128, 2, 86, 473, 3, 336, 34, 64, 364, 379, 1939, 155, 526, 2116, 92, 3486, 1064, 102, 459, 4971, 49, 1929, 2, 3, 156, 114, 2849, 370, 159, 547, 2603, 53, 277, 3323, 802, 183, 498, 797, 326, 7418, 684, 1438, 201, 1939, 56, 3, 2007, 873, 55, 137, 265, 364, 24, 10, 5, 7419, 2476, 241, 168, 99, 105, 22, 3476, 539, 5, 91, 574, 1133, 2, 2, 2104, 1438, 1, 98, 91, 1396, 2, 590, 6318, 6, 1939, 718, 423]\n",
      "14771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[   0,    0,    0, ...,  406, 3108,  310],\n",
       "        [   0,    0,    0, ..., 2704,   18,  121],\n",
       "        [   0,    0,    0, ...,   10,    5,  332],\n",
       "        ...,\n",
       "        [   0,    0,    0, ...,    4,  645,  771],\n",
       "        [   0,    0,    0, ...,  964,  606,    1],\n",
       "        [   0,    0,    0, ...,   57,  101, 1004]]),\n",
       " array([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 0, 1, 1, 0, 0, 0, 0], dtype=int64))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_cut = x[1001:1101]\n",
    "max_length_test = checkLength(10000,x)\n",
    "#featWR,targetWR,tokenizerr = beforeCreateModel(max_feat=10000,dataset=x,max_length=max_length) #fulldic max_feat = 14000\n",
    "featWR,targetWR,tokenizerr = beforeCreateModel(max_feat=10000,dataset=x,max_length=1304)\n",
    "\n",
    "featWR,targetWR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1304"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featWR.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(featWR,targetWR, test_size = 0.2, train_size = 0.8, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9999682 ],\n",
       "       [0.23094516],\n",
       "       [0.00022506],\n",
       "       [0.9218977 ],\n",
       "       [0.99997437],\n",
       "       [0.9999995 ],\n",
       "       [0.2818756 ],\n",
       "       [0.9296115 ],\n",
       "       [0.9985586 ],\n",
       "       [0.9778511 ],\n",
       "       [0.17688665],\n",
       "       [0.00004893],\n",
       "       [0.99959546],\n",
       "       [0.94174314],\n",
       "       [0.99802953],\n",
       "       [0.19212028],\n",
       "       [0.12460896],\n",
       "       [0.88474685],\n",
       "       [0.99976665],\n",
       "       [0.999979  ],\n",
       "       [0.0168418 ],\n",
       "       [0.00007791],\n",
       "       [0.10732297],\n",
       "       [0.0087539 ],\n",
       "       [0.99710923],\n",
       "       [0.9854364 ],\n",
       "       [0.61839557],\n",
       "       [0.01905701],\n",
       "       [0.00025873],\n",
       "       [0.02852048],\n",
       "       [0.6722403 ],\n",
       "       [0.97459793],\n",
       "       [0.99958473],\n",
       "       [0.00010738],\n",
       "       [0.9980806 ],\n",
       "       [0.9955041 ],\n",
       "       [0.9993445 ],\n",
       "       [0.00759503],\n",
       "       [0.996567  ],\n",
       "       [0.00004767],\n",
       "       [0.97916573],\n",
       "       [0.6430848 ],\n",
       "       [0.00000292],\n",
       "       [0.496031  ],\n",
       "       [0.03514318],\n",
       "       [0.00051279],\n",
       "       [0.9996737 ],\n",
       "       [0.22017501],\n",
       "       [0.00031256],\n",
       "       [0.00101884],\n",
       "       [0.999488  ],\n",
       "       [0.00000408],\n",
       "       [0.95729744],\n",
       "       [0.37051892],\n",
       "       [0.9949014 ],\n",
       "       [0.9865936 ],\n",
       "       [0.09379231],\n",
       "       [0.998154  ],\n",
       "       [0.98449486],\n",
       "       [0.99587077],\n",
       "       [0.00058644],\n",
       "       [0.98412204],\n",
       "       [0.03108827],\n",
       "       [0.00435978],\n",
       "       [0.00048147],\n",
       "       [0.9941081 ],\n",
       "       [0.0013354 ],\n",
       "       [0.00001701],\n",
       "       [0.9999963 ],\n",
       "       [0.00002823],\n",
       "       [0.00031315],\n",
       "       [0.01410522],\n",
       "       [0.95388025],\n",
       "       [0.15101391],\n",
       "       [0.7168937 ],\n",
       "       [0.99999344],\n",
       "       [0.0003923 ],\n",
       "       [0.9510537 ],\n",
       "       [0.00232027],\n",
       "       [0.11855143],\n",
       "       [0.00034302],\n",
       "       [0.567093  ],\n",
       "       [0.98905003],\n",
       "       [0.16634734],\n",
       "       [0.9993179 ],\n",
       "       [0.00069375],\n",
       "       [0.93926996],\n",
       "       [0.00145084],\n",
       "       [0.00831305],\n",
       "       [0.998367  ],\n",
       "       [0.2795634 ],\n",
       "       [0.9929789 ],\n",
       "       [0.6696164 ],\n",
       "       [0.00003564],\n",
       "       [0.00113778],\n",
       "       [0.75271165],\n",
       "       [0.00061597],\n",
       "       [0.9877935 ],\n",
       "       [0.9757009 ],\n",
       "       [0.96383524],\n",
       "       [0.9944636 ],\n",
       "       [0.00110821],\n",
       "       [0.8394093 ],\n",
       "       [0.00008814],\n",
       "       [0.00177321],\n",
       "       [0.99991477],\n",
       "       [0.13242969],\n",
       "       [0.05515537],\n",
       "       [0.9889171 ],\n",
       "       [0.0073222 ],\n",
       "       [0.07982535],\n",
       "       [0.9906894 ],\n",
       "       [0.49225447],\n",
       "       [0.00015915],\n",
       "       [0.01706545],\n",
       "       [0.00056753],\n",
       "       [0.9998667 ],\n",
       "       [0.9995043 ],\n",
       "       [0.9979195 ],\n",
       "       [0.3099144 ],\n",
       "       [0.21022475],\n",
       "       [0.9195911 ],\n",
       "       [0.0041329 ],\n",
       "       [0.00002575],\n",
       "       [0.00156962],\n",
       "       [0.9093553 ],\n",
       "       [0.9777909 ],\n",
       "       [0.00472546],\n",
       "       [0.00125484],\n",
       "       [0.0082151 ],\n",
       "       [0.9996816 ],\n",
       "       [0.00004819],\n",
       "       [0.33000812],\n",
       "       [0.25823647],\n",
       "       [0.00053725],\n",
       "       [0.8960959 ],\n",
       "       [0.06071926],\n",
       "       [0.00345931],\n",
       "       [0.00234129],\n",
       "       [0.00000264],\n",
       "       [0.00574467],\n",
       "       [0.09329735],\n",
       "       [0.985385  ],\n",
       "       [0.8211433 ],\n",
       "       [0.95983607],\n",
       "       [0.00015664],\n",
       "       [0.3326085 ],\n",
       "       [0.98153263],\n",
       "       [0.00098686],\n",
       "       [0.00026874],\n",
       "       [0.00212137],\n",
       "       [0.9120503 ],\n",
       "       [0.13816257],\n",
       "       [0.00035251],\n",
       "       [0.9641947 ],\n",
       "       [0.00002432],\n",
       "       [0.00188291],\n",
       "       [0.00019976],\n",
       "       [0.00002807],\n",
       "       [0.00007437],\n",
       "       [0.00533301],\n",
       "       [0.00736118],\n",
       "       [0.00000686],\n",
       "       [0.05097684],\n",
       "       [0.014966  ],\n",
       "       [0.05721886],\n",
       "       [0.00073036],\n",
       "       [0.19568737],\n",
       "       [0.9999646 ],\n",
       "       [0.9989139 ],\n",
       "       [0.0372673 ],\n",
       "       [0.42357653],\n",
       "       [0.02832998],\n",
       "       [0.66876495],\n",
       "       [0.7947589 ],\n",
       "       [0.900682  ],\n",
       "       [0.00411458],\n",
       "       [0.9999862 ],\n",
       "       [0.9940399 ],\n",
       "       [0.00374761],\n",
       "       [0.00305145],\n",
       "       [0.00590058],\n",
       "       [0.9993262 ],\n",
       "       [0.04524427],\n",
       "       [0.9974312 ],\n",
       "       [0.99999905],\n",
       "       [0.99982834],\n",
       "       [0.00811635],\n",
       "       [0.00487643],\n",
       "       [0.99420255],\n",
       "       [0.00001754],\n",
       "       [0.00407   ],\n",
       "       [0.92876166],\n",
       "       [0.99949324],\n",
       "       [0.00284384],\n",
       "       [0.9408336 ],\n",
       "       [0.8005261 ],\n",
       "       [0.19427167],\n",
       "       [0.9994543 ],\n",
       "       [0.00003037]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#resultWR = testWR.predict(featWR)\n",
    "resultWR = testWR.predict(X_test)\n",
    "\n",
    "resultWR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[49 55]\n",
      " [64 32]]\n"
     ]
    }
   ],
   "source": [
    "showConfusionMatrix(trueLabel=Y_test,resultToShow=resultWR)\n",
    "#confusion_matrix(target[:,1] , result>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showWordWithCode(dataToMap=featWR , tokenizer=tokenizerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 - 2s - loss: 3.2029 - accuracy: 0.4050\n",
      "score: 3.20\n",
      "acc: 0.41\n"
     ]
    }
   ],
   "source": [
    "evaluateModel(testWR,feat=X_test,target=Y_test,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1304, 128)         1280000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,411,713\n",
      "Trainable params: 1,411,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "testWR.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6(tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
