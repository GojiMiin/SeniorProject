{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_result = pd.read_csv(\"../Clean/lemma_result.csv\") #Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      cleaned_review  Label\n",
      "0  one reviewers mention watch 1 oz episode hook ...      1\n",
      "1  wonderful little production film technique una...      1\n",
      "2  think wonderful way spend time hot summer week...      1\n",
      "3  basically family little boy jake think zombie ...      0\n",
      "4  petter mattei love time money visually stun fi...      1\n"
     ]
    }
   ],
   "source": [
    "print(lemma_result.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(lemma_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wonderful little production film technique unassuming old time bbc fashion give comfort sometimes discomforting sense realism entire piece actors extremely well choose michael sheen get polari voice pat truly see seamless edit guide reference williams diary entries well worth watch terrificly write perform piece masterful production one great master comedy life realism really come home little things fantasy guard rather use traditional dream techniques remain solid disappear play knowledge sense particularly scenes concern orton halliwell set particularly flat halliwell murals decorate every surface terribly well do'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(lemma_result['cleaned_review'])\n",
    "\n",
    "lemma_result['cleaned_review'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1542468096810376]\n"
     ]
    }
   ],
   "source": [
    "print([X[1, tfidf.vocabulary_['discomforting']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    499\n",
       "1    501\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_result.groupby('Label').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = lemma_result.cleaned_review\n",
    "y = lemma_result.Label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has total 750 entries with 48.27% negative, 51.73% positive\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(X_train),\n",
    "                                                                             (len(X_train[y_train == 0]) / (len(X_train)*1.))*100,\n",
    "                                                                            (len(X_train[y_train == 1]) / (len(X_train)*1.))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout,Dropout_U,Dropout_W\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fatures = 3000\n",
    "tokenizer = Tokenizer(nb_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(lemma_result['cleaned_review'].values)\n",
    "X1 = tokenizer.texts_to_sequences(lemma_result['cleaned_review'].values)\n",
    "X1 = pad_sequences(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 505) (750, 2)\n",
      "(250, 505) (250, 2)\n"
     ]
    }
   ],
   "source": [
    "Y1 = pd.get_dummies(lemma_result['Label']).values\n",
    "X1_train, X1_test, Y1_train, Y1_test = train_test_split(X1,Y1, random_state = 42)\n",
    "print(X1_train.shape,Y1_train.shape)\n",
    "print(X1_test.shape,Y1_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 505, 150)          450000    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 505, 150)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               280800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 731,202\n",
      "Trainable params: 731,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 150\n",
    "lstm_out = 200\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X1.shape[1]))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 750 samples\n",
      "Epoch 1/10\n",
      "750/750 - 24s - loss: 0.6910 - accuracy: 0.5093\n",
      "Epoch 2/10\n",
      "750/750 - 23s - loss: 0.5883 - accuracy: 0.7707\n",
      "Epoch 3/10\n",
      "750/750 - 23s - loss: 0.3347 - accuracy: 0.9133\n",
      "Epoch 4/10\n",
      "750/750 - 21s - loss: 0.1953 - accuracy: 0.9307\n",
      "Epoch 5/10\n",
      "750/750 - 21s - loss: 0.0943 - accuracy: 0.9800\n",
      "Epoch 6/10\n",
      "750/750 - 21s - loss: 0.0242 - accuracy: 0.9947\n",
      "Epoch 7/10\n",
      "750/750 - 21s - loss: 0.0103 - accuracy: 0.9987\n",
      "Epoch 8/10\n",
      "750/750 - 22s - loss: 0.0079 - accuracy: 0.9987\n",
      "Epoch 9/10\n",
      "750/750 - 20s - loss: 0.0139 - accuracy: 0.9973\n",
      "Epoch 10/10\n",
      "750/750 - 21s - loss: 0.0248 - accuracy: 0.9973\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "history = model.fit(X1_train, Y1_train, epochs = 10, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 - 1s - loss: 0.8859 - accuracy: 0.6800\n",
      "score: 0.89\n",
      "acc: 0.68\n"
     ]
    }
   ],
   "source": [
    "score,acc = model.evaluate(X1_test, Y1_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6(tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
